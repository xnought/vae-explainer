{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass\n",
    "\n",
    "At a high level, a VAE is just an Autoencoder with some extra stuff to make the latent space better.\n",
    "\n",
    "An autoencoder $f(x)$ simply takes an input $x$ and produces a reconstruction of $x$ as $\\hat{x}$. It distills $x$ into less numbers as a compression (bottleneck) as $$f(x) = \\text{decode}(\\text{encode}(x))$$ where encode and decode are both learnable functions.\n",
    "\n",
    "With VAEs, the encode function produces a probability distribution rather than numbers that the decoder uses directly. At the encoding step, we use the probability distribution to randomly sample the vector, which then gets decoded by the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.ibm.com/think/topics/variational-autoencoder#:~:text=Variational%20autoencoders%20(VAEs)%20are%20generative,other%20autoencoders%2C%20such%20as%20denoising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
