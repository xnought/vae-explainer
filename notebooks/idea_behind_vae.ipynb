{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass\n",
    "\n",
    "At a high level, a VAE is just an Autoencoder with some extra stuff to make the latent space better.\n",
    "\n",
    "An autoencoder $f(x)$ simply takes an input $x$ and produces a reconstruction of $x$ as $\\hat{x}$. It distills $x$ into less numbers as a compression (bottleneck) as $$f(x) = \\text{decode}(\\text{encode}(x))$$ where encode and decode are both learnable functions.\n",
    "\n",
    "With VAEs, the encode function produces a probability distribution rather than numbers that the decoder uses directly. At the encoding step, we use the probability distribution to randomly sample the vector, which then gets decoded by the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[ 0.0257, -0.3268,  0.1138,  0.0248, -0.2998, -0.8924,  0.7382,  0.0986,\n",
      "         -0.4729, -0.5824]])\n",
      "x_hat tensor([[ 0.3332,  0.2883,  0.1484, -0.4554,  0.0061,  0.2386, -0.0728, -0.1943,\n",
      "         -0.0406, -0.1597]])\n",
      "reconstruction loss tensor(0.3186)\n"
     ]
    }
   ],
   "source": [
    "# Example of untrained vanilla autoencoder\n",
    "# with an input of 10 numbers and tries to reconstruct 10 numbers (not very well since not trained)\n",
    "with torch.no_grad():\n",
    "\tencoder = torch.nn.Sequential(\n",
    "\t\tnn.Linear(10, 5),\n",
    "\t\tnn.ReLU(),\n",
    "\t\tnn.Linear(5, 2),\n",
    "\t)\n",
    "\tdecoder = nn.Sequential(\n",
    "\t\tnn.Linear(2, 5),\n",
    "\t\tnn.ReLU(),\n",
    "\t\tnn.Linear(5, 10),\n",
    "\t)\n",
    "\tx = torch.randn((1, 10))\n",
    "\tf = lambda x: decoder(encoder(x))\n",
    "\tx_hat = f(x)\n",
    "\tprint(\"x\", x)\n",
    "\tprint(\"x_hat\", x_hat)\n",
    "\tprint(\"reconstruction loss\", F.mse_loss(x_hat, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pass\n",
    "\n",
    "Why does turning the bottleneck into a probability distribution help? And how can we represent that in math and pytorch?\n",
    "\n",
    "The logic is that Autoencoders can embed their vectors anywhere in space. So the output shape might be terrible for us humans to reason about. Additionally, vector math and interpolation between vectors in the latent space, might not be as smooth as we'd want. \n",
    "\n",
    "We humans deal with lines, whereas the autoencoder can embed a very non-linear latent space that is understandable to the autoencoder, but not to us.\n",
    "\n",
    "> [!NOTE]\n",
    "> You could in theory run a secondary Sparse Autoencoder (SAE) to interpolate a vanilla autoencoders latent space if it's not great to look at. This is redundant. Let's just use a VAE.\n",
    "\n",
    "In essence, instead of outputting a vector that is directly decoded as $$\\vec{v} = \\begin{bmatrix}v_0\\\\v_1\\\\\\vdots\\\\v_n\\end{bmatrix}$$ where $n$ is the size of the latent space $\\mathbb{R}^n$, we instead can represent the output as the parameters in a probability distribution and take $\\vec{v}$ as a sample from the constructed distribution.\n",
    "\n",
    "For example, we could output two vectors in the latent space as\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\vec{\\mu} &= \\begin{bmatrix}\\mu_0\\\\\\mu_1\\\\\\vdots\\\\\\mu_i\\end{bmatrix}\\\\\n",
    "\t\\vec{\\sigma} &= \\begin{bmatrix}\\sigma_0\\\\\\sigma_1\\\\\\vdots\\\\\\sigma_i\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where each pair of matching indices from $\\vec{\\mu}$ and $\\vec{\\sigma}$ are then sampled from a Gaussian distribution to recreate the $\\vec{v}$ as\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\vec{v} &= \\begin{bmatrix}\n",
    "\t\t\t\t\tv_0 \\sim N(\\mu_0, \\sigma_0^2)\\\\\n",
    "\t\t\t\t\tv_1 \\sim N(\\mu_1, \\sigma_1^2)\\\\\n",
    "\t\t\t\t\t\\vdots\\\\\n",
    "\t\t\t\t\tv_n \\sim N(\\mu_n, \\sigma_n^2)\\\\\n",
    "\t\t\t\t\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "using a random sampler during inference time. This way, each value in $\\vec{v}$ is a continuous function with a value realized at inference time. The latent space created from these outputs looks much more normally distributed (duh) and nicer to work with and do vector math on. Making the VAE more useful in more cases than the regular AE.\n",
    "\n",
    "Note that the standard deviations need to be positive so we can apply an activation like ReLU or softplus to the standard deviation functions in the latent space before sampling.\n",
    "\n",
    "Note that since we sample to create the vector, we have to create a pathway for the gradient that doesn't terminate. How does one backprop through a random number generator? The workaround is called the reparameterization trick.\n",
    "\n",
    "This is where use the fact that a normal distribution can be represented as the standard deviation times a standard normal distribution + the mean. So we have that $$\\vec{v} = \\mu + \\sigma \\epsilon$$ where $\\epsilon \\sim N(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallVAE(\n",
      "  (enc_fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (enc_mu): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (enc_sig): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (dec_fc1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (dec_fc2): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n",
      "x tensor([[-1.0133,  0.0472,  1.4174, -0.8698,  0.8424,  1.5832,  1.2412,  1.5544,\n",
      "          1.1725,  0.2692]])\n",
      "x_hat tensor([[ 0.9043,  0.1571,  0.1047,  0.2012, -0.4362,  0.5704, -0.1553,  0.0706,\n",
      "         -0.5312,  0.2040]])\n"
     ]
    }
   ],
   "source": [
    "# now let me change the example before to include the normal distributions \n",
    "class SmallVAE(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# encoder\n",
    "\t\tself.enc_fc1 = nn.Linear(10, 5)\n",
    "\t\t# prob dist (replaces the latent embedder)\n",
    "\t\tself.enc_mu = nn.Linear(5, 2)\n",
    "\t\tself.enc_sig = nn.Linear(5, 2)\n",
    "\n",
    "\t\t# decoder\n",
    "\t\tself.dec_fc1 = nn.Linear(2, 5)\n",
    "\t\tself.dec_fc2 = nn.Linear(5, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# encoder\n",
    "\t\tx = self.enc_fc1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tmu, sig = self.enc_mu(x), self.enc_sig(x)\n",
    "\t\tsig = F.softplus(sig) # standard deviation must be positive, mean can be anything\n",
    "\n",
    "\t\teps = torch.randn_like(sig) # standard normal\n",
    "\t\tx = mu + sig*eps # reconstruct the vector from sample\n",
    "\n",
    "\t\t# decoder\n",
    "\t\tx = self.dec_fc1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx = self.dec_fc2(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "with torch.no_grad():\n",
    "\tf_vae = SmallVAE()\n",
    "\tx = torch.randn((1, 10))\n",
    "\tx_hat = f_vae(x)\n",
    "\n",
    "\tprint(f_vae)\n",
    "\tprint(\"x\", x)\n",
    "\tprint(\"x_hat\", x_hat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.ibm.com/think/topics/variational-autoencoder#:~:text=Variational%20autoencoders%20(VAEs)%20are%20generative,other%20autoencoders%2C%20such%20as%20denoising.\n",
    "- https://www.youtube.com/watch?v=9zKuYvjFFS8\n",
    "- https://arxiv.org/abs/1312.6114 (original VAE paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
